# ğŸ¤– AI Live Chat Agent

A mini full-stack web app that simulates a customer support chat widget powered by an AI agent.

The app lets users chat with an AI support agent, persists conversations, and answers questions using domain knowledge of a fictional e-commerce store.

---

## âœ¨ Features

* ğŸ’¬ Live chat UI (user vs AI messages)
* ğŸ§  AI agent powered by LLM (Groq + Llama3)
* ğŸ—‚ï¸ Conversation & message persistence (PostgreSQL)
* ğŸ” Session-based chat history
* âš™ï¸ Robust backend with validation & error handling
* ğŸ¨ Simple, clean frontend UX
* ğŸ” No secrets committed

---

## ğŸ§± Tech Stack

**Backend**

* Node.js + TypeScript
* Express
* Prisma ORM
* PostgreSQL (hosted on Render)
* Zod for validation

**Frontend**

* SvelteKit
* TypeScript

**LLM**

* Groq API (Llama3-8B)

---

## ğŸ“ Project Structure

```
ai-chat/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ routes/      # API routes
â”‚   â”‚   â”œâ”€â”€ services/   # LLM integration
â”‚   â”‚   â”œâ”€â”€ db/         # Prisma client
â”‚   â”‚   â””â”€â”€ index.ts
â”‚   â”œâ”€â”€ prisma/
â”‚   â”‚   â””â”€â”€ schema.prisma
â”‚   â”œâ”€â”€ prisma.config.ts
â”‚   â””â”€â”€ .env
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ src/routes/+page.svelte
â”‚
â””â”€â”€ README.md
```

---

## ğŸš€ How to Run Locally

### âœ… Prerequisites

* Node.js â‰¥ 18
* npm
* A PostgreSQL DB (Render used here)
* Groq API key

---

### 1ï¸âƒ£ Clone Repo

```bash
git clone https://github.com/tangentveil/ai-chat-groq.git
cd ai-chat-groq
```

---

### 2ï¸âƒ£ Backend Setup

```bash
cd backend
npm install
```

Create `.env`:

```env
PORT=4000
DATABASE_URL="postgresql://user:password@host:5432/dbname?sslmode=require"
GROQ_API_KEY=your_groq_api_key
```

> Use the **External Database URL** from Render and ensure `?sslmode=require` is present.

---

### 3ï¸âƒ£ Prisma Setup

```bash
npx prisma generate
npx prisma migrate dev --name init
```

(Optional) Open Prisma Studio:

```bash
npx prisma studio
```

---

### 4ï¸âƒ£ Start Backend

```bash
npm run dev
```

Backend runs on:
ğŸ‘‰ [http://localhost:4000](http://localhost:4000)

---

### 5ï¸âƒ£ Frontend Setup

```bash
cd ../frontend
npm install
npm run dev
```

Frontend runs on:
ğŸ‘‰ [http://localhost:5173](http://localhost:5173)

---

### 6ï¸âƒ£ Open App

Visit:

```
http://localhost:5173
```

Start chatting with the AI agent!

---

## ğŸ”Œ API

### `POST /chat/message`

**Request**

```json
{
  "message": "What is your return policy?",
  "sessionId": "optional-uuid"
}
```

**Response**

```json
{
  "reply": "We offer a 7-day return window...",
  "sessionId": "uuid"
}
```

---

## ğŸ—„ï¸ Database Schema

```prisma
model Conversation {
  id        String   @id @default(uuid())
  createdAt DateTime @default(now())
  messages  Message[]
}

model Message {
  id             String   @id @default(uuid())
  conversationId String
  sender         Sender
  text           String
  createdAt      DateTime @default(now())

  conversation Conversation @relation(fields: [conversationId], references: [id])
}

enum Sender {
  user
  ai
}
```

---

## ğŸ§  LLM Integration

**Provider:** Groq
**Model:** `openai/gpt-oss-20b`

Chosen because:

* Free tier
* Very fast inference
* OpenAI-compatible chat API

**Prompting Strategy:**

* System prompt defines agent as helpful e-commerce support bot
* Hardcoded FAQ knowledge:

  * Shipping policy
  * Returns & refunds
  * Support hours
* Last ~10 messages included as context

**Guardrails:**

* Token cap (`max_tokens: 300`)
* Low temperature for concise replies
* Graceful fallback on API errors

---

## ğŸ—ï¸ Architecture Overview

**Backend layers:**

* `routes/` â†’ HTTP handling & validation
* `services/` â†’ LLM logic
* `db/` â†’ Prisma client
* Clear separation of concerns

**Flow:**

1. Frontend sends message
2. Backend validates input
3. Conversation fetched/created
4. Message persisted
5. LLM called with history + prompt
6. AI reply stored & returned

**Extensibility:**

* Easy to plug new channels (WhatsApp, IG, etc.) by reusing services
* LLM logic isolated in one service
* DB schema supports multi-session chats

---

## ğŸ›¡ï¸ Robustness

* Input validation with Zod
* Empty/long messages blocked
* LLM failures handled gracefully
* No backend crashes on bad input
* Secrets via env vars only

---

## ğŸ¨ UX Notes

* Auto-scroll chat
* Clear user vs AI styling
* Disabled send during request
* â€œAgent is typingâ€¦â€ indicator
* Session persisted in `localStorage`

---

## ğŸ§ª Known Limitations

* No authentication
* No admin tools / dashboard
* No streaming responses
* Minimal styling

---

## ğŸ§¾ Trade-offs

* Used Groq instead of OpenAI due to free access
* Hardcoded FAQ for speed instead of vector DB
* Remote Postgres used even for local dev for simplicity
* Simple Express server instead of full framework

---

## ğŸ™Œ Author

Built by **Anirudha Rajodiya**.